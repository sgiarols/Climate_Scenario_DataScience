{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy import special\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from scipy.stats.mstats import trim as trim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "\n",
    "This is a set of utility functions: transpositions and count occurrencies\n",
    "\n",
    "World databases are displayed in billion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransformUN\u001b[39m(indata: \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame, region: \u001b[38;5;28mstr\u001b[39m, rename: \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124;03m\"\"\"Transforms United Nations (UN) data having horizontal series\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    of years into a vertical series of years. It receives:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    indata: dataframe of probabilistic/deterministic UN data where\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    Returns: DataFrame with data for each years on rows\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    with columns Population, Sccenario, Years\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     ddata \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(indata\u001b[38;5;241m.\u001b[39mloc[indata\u001b[38;5;241m.\u001b[39mIPCCRegion\u001b[38;5;241m==\u001b[39mregion])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "def transformUN(indata: pd.DataFrame, region: str, rename: bool):\n",
    "    \n",
    "    \"\"\"Transforms United Nations (UN) data having horizontal series\n",
    "    of years into a vertical series of years. It receives:\n",
    "    indata: dataframe of probabilistic/deterministic UN data where\n",
    "    data for each year is a new column\n",
    "    region: name of the region selected\n",
    "    rename: if True, raw labels would be changes into corresponding\n",
    "    cumulative probability numbers\n",
    "    select True for UN probability projections\n",
    "    \n",
    "    Returns: DataFrame with data for each years on rows\n",
    "    with columns Population, Sccenario, Years\"\"\"\n",
    "\n",
    "    ddata = pd.DataFrame(indata.loc[indata.IPCCRegion==region])\n",
    "\n",
    "    if \"Variant\" in ddata.columns:\n",
    "        ddata = ddata.drop(\"Variant\", axis=1)\n",
    "\n",
    "    columns = ddata.Scenario.to_list()\n",
    "\n",
    "    wt = ddata.transpose()\n",
    "\n",
    "    wt = wt.reset_index()\n",
    "\n",
    "    wtt = wt[3:]\n",
    "    \n",
    "    wtt.columns = [\"year\"] + columns\n",
    "    \n",
    "    wnew = pd.DataFrame()\n",
    "    for c, col in enumerate(columns):\n",
    "        data = pd.DataFrame(wtt[wtt.columns[0]])\n",
    "        data = data.rename(columns={data.columns[0]: \"Year\"})\n",
    "        data[\"Population\"] = wtt[wtt.columns[c+1]]\n",
    "        data[\"Scenario\"] = wtt.columns[c+1]\n",
    "        wnew = pd.concat((wnew, data), axis=0)\n",
    "\n",
    "    if rename:\n",
    "        change={\"Lower 80\": \"20\", \"Lower 95\": \"5\", \"Median\": \"50\", \"Upper 80\": \"80\", \"Upper 95\": \"95\"}\n",
    "        for ch in change.keys():\n",
    "            wnew[\"Scenario\"] = wnew[\"Scenario\"].replace(ch, change[ch])\n",
    "                    \n",
    "    return (wnew)\n",
    "\n",
    "def transformUNminmax(indata: pd.DataFrame, region: str, rename: bool):\n",
    "\n",
    "    \"\"\"Transforms UN data in vertical series\n",
    "    for scenarios with max and min population in 2100\n",
    "    indata: dataframe of UN data where data for each year is a new column\n",
    "    region: name of a region selected\n",
    "    rename: if True, raw labels would be changes into corresponding cumulative probability numbers\n",
    "    select True for UN probability projections\n",
    "    \n",
    "    Returns: DataFrame with \"\"\"\n",
    "\n",
    "    sel_series = [] #list to append the scenrios fulfilling requirements\n",
    "    undata = transformUN(indata, region, rename)\n",
    "    max_2100 = pd.DataFrame(undata.loc[undata.Year==2100][\"Population\"]).max()\n",
    "    sel_series.append(list(set(undata.loc[undata.Population==max_2100.values[0]][\"Scenario\"]))[0])\n",
    "    min_2100 = pd.DataFrame(undata.loc[undata.Year==2100][\"Population\"]).min()\n",
    "    sel_series.append(list(set(undata.loc[undata.Population==min_2100.values[0]][\"Scenario\"]))[0])\n",
    "    undata = pd.DataFrame(undata.loc[undata.Scenario.isin(sel_series)])\n",
    "    return undata, sel_series\n",
    "\n",
    "def transformAR(indata: pd.DataFrame, region: str, dataset: str, threshold: tuple, years: list,\n",
    "                variable: str, renamevariable: str):\n",
    "\n",
    "    \"\"\"Transforms indata from assessment reports\n",
    "    (ARs) from having years in columns to having years in rows\n",
    "    indata: dataframe of AR data where data for each year is a new column\n",
    "    region: name of a region selected\n",
    "    dataset: label to rename the series (e.g. AR6)\n",
    "    threshold: ranges used to filter out the data using quantiles\n",
    "    years: list of years to represent\n",
    "\n",
    "    Returns: DataFrame of AR data with columns renamevariable (i.e. Population or GDP), Region, Series, Year, Unit, string Year\n",
    "    \"\"\"\n",
    "    ar = pd.DataFrame(indata.loc[indata.Variable == variable])\n",
    "    ared = pd.DataFrame(ar.loc[ar.Region == region])\n",
    "    arnew = pd.DataFrame()\n",
    "    for year in years:\n",
    "        data = pd.DataFrame(np.vstack((ared[str(year)], ared[\"Unit\"])).transpose(), columns = [renamevariable, \"Unit\"])\n",
    "        data[\"Year\"] = year\n",
    "        data[\"sYear\"] = str(year)\n",
    "        low = data[renamevariable].quantile(threshold[0])\n",
    "        high = data[renamevariable].quantile(threshold[1])\n",
    "        data = pd.DataFrame(data.loc[data[renamevariable] >= low])\n",
    "        data = pd.DataFrame(data.loc[data[renamevariable] < high])\n",
    "        data[\"Series\"] = dataset\n",
    "        arnew = pd.concat((arnew,data))\n",
    "    return arnew\n",
    "\n",
    "def transformSSP(indata, region, years, variable):\n",
    "\n",
    "    \"\"\"Transforms Shared Socio-economic Pathways (SSP)\n",
    "    data from years in horizontal series to years in vertical series\n",
    "    indata: DataFrame with SSP population, following IPCC region conventions\n",
    "    region: selected IPCC region\n",
    "    years: selected years\n",
    "    variable: Population\n",
    "\n",
    "    Returns: DataFrame with columns sYears, Population, Model, Scenario\n",
    "    \"\"\"\n",
    "\n",
    "    syears = [str(y) for y in years]\n",
    "\n",
    "    columns = [\"Model\"] + [\"Scenario\"] + [\"IPCCRegion\"] + syears\n",
    "\n",
    "    indata = indata.groupby([\"Model\", \"Scenario\",\"IPCCRegion\"])[syears].sum().reset_index()\n",
    "\n",
    "    wout = pd.DataFrame()\n",
    "    models = list(set(indata.Model))\n",
    "    scenarios = [\"SSP1\", \"SSP2\", \"SSP3\", \"SSP4\", \"SSP5\"]\n",
    "    for model in models:\n",
    "        for scenario in scenarios:\n",
    "            wt = pd.DataFrame(indata.loc[(indata.IPCCRegion == region) \n",
    "                            & (indata.Model == model) \n",
    "                            & (indata.Scenario==scenario)])\n",
    "            \n",
    "            wt = wt[columns]\n",
    "            wt.columns = [\"Model\", \"Scenario\", \"IPCCRegion\"] + syears\n",
    "            wtt = wt.transpose()\n",
    "            wts =  pd.DataFrame(wtt.loc[wtt.index.isin(syears)])\n",
    "            wts.columns = [variable]\n",
    "            wts[\"Model\"] = model\n",
    "            wts[\"Scenario\"] = scenario\n",
    "            wts[\"Year\"] = years\n",
    "            wout = pd.concat((wout, wts))\n",
    "    wout = wout.reset_index()\n",
    "    wout = wout.drop(columns=[\"index\"])\n",
    "    wout = wout[[\"Year\", variable, \"Model\", \"Scenario\"]]\n",
    "    return (wout)\n",
    "\n",
    "def readSSP(years: list):\n",
    "\n",
    "    \"\"\"Read Shared Socioeconomic Pathays data from folder data\\SSP\n",
    "    Receives:\n",
    "    - years: list of years \n",
    "    Returns:\n",
    "    SSPpop: A dataframe with SSP population\n",
    "    SSPgdp: A dataframe with SSP GDP,\n",
    "    SSPgroups: A dataframe with SSP population, grouped by MODEL and SCENARIO\n",
    "    SSPgdpgroups: A dataframe with SSP GDP, grouped by MODEL and SCENARIO\n",
    "    codes: dictionary with ISO code and country names\n",
    "    scenarios: dictionary scenario names and variants\n",
    "    \"\"\"\n",
    "\n",
    "    codefile  = \"codes_and_country_names.csv\" \n",
    "    sspfile = \"SspDb_country_data_2013-06-12.csv\"\n",
    "    folder = r'data/SSP'\n",
    "    incodes = pd.read_csv(os.path.join(folder, codefile))\n",
    "    SSP = pd.read_csv(os.path.join(folder, sspfile))\n",
    "\n",
    "    SSP[\"Country\"] = SSP[\"REGION\"]\n",
    "    columns = SSP.columns[:5].to_list() +[\"Country\"] + [str(y) for y in years]\n",
    "    SSP = SSP[columns]\n",
    "    regions = list(set(SSP[\"REGION\"]))\n",
    "    incodes.columns = [\"ISO\", \"Country\"]\n",
    "\n",
    "    keys=list(incodes.ISO)\n",
    "    values=list(incodes.Country)\n",
    "    codes = {k:values[keys.index(k)] for k in keys}\n",
    "\n",
    "    UNreg  =       [\"Burundi\",\n",
    "                    \"Comoros\",\n",
    "                    \"Djibouti\",\n",
    "                    \"Eritrea\",\n",
    "                    \"Ethiopia\",\n",
    "                    \"Kenya\",\n",
    "                    \"Madagascar\",\n",
    "                    \"Malawi\",\n",
    "                    \"Mauritius\",\n",
    "                    \"Mayotte\",\n",
    "                    \"Mozambique\",\n",
    "                    \"Réunion\",\n",
    "                    \"Rwanda\",\n",
    "                    \"Seychelles\",\n",
    "                    \"Somalia\",\n",
    "                    \"South Sudan\",\n",
    "                    \"Uganda\",\n",
    "                    \"United Republic of Tanzania\",\n",
    "                    \"Zambia\",\n",
    "                    \"Zimbabwe\",\n",
    "                    \"Angola\",\n",
    "                    \"Cameroon\",\n",
    "                    \"Central African Republic\",\n",
    "                    \"Chad\",\n",
    "                    \"Congo\",\n",
    "                    \"Democratic Republic of the Congo\",\n",
    "                    \"Equatorial Guinea\",\n",
    "                    \"Gabon\",\n",
    "                    \"Sao Tome and Principe\",\n",
    "                    \"Algeria\",\n",
    "                    \"Egypt\",\n",
    "                    \"Libya\",\n",
    "                    \"Morocco\",\n",
    "                    \"Sudan\",\n",
    "                    \"Tunisia\",\n",
    "                    \"Western Sahara\",\n",
    "                    \"Botswana\",\n",
    "                    \"Eswatini\",\n",
    "                    \"Lesotho\",\n",
    "                    \"Namibia\",\n",
    "                    \"South Africa\",\n",
    "                    \"Benin\",\n",
    "                    \"Burkina Faso\",\n",
    "                    \"Cabo Verde\",\n",
    "                    \"Côte d'Ivoire\",\n",
    "                    \"Gambia\",\n",
    "                    \"Ghana\",\n",
    "                    \"Guinea\",\n",
    "                    \"Guinea-Bissau\",\n",
    "                    \"Liberia\",\n",
    "                    \"Mali\",\n",
    "                    \"Mauritania\",\n",
    "                    \"Niger\",\n",
    "                    \"Nigeria\",\n",
    "                    \"Saint Helena\",\n",
    "                    \"Senegal\",\n",
    "                    \"Sierra Leone\",\n",
    "                    \"Togo\",\n",
    "                    \"Kazakhstan\",\n",
    "                    \"Kyrgyzstan\",\n",
    "                    \"Tajikistan\",\n",
    "                    \"Turkmenistan\",\n",
    "                    \"Uzbekistan\",\n",
    "                    \"China\",\n",
    "                    \"China, Hong Kong SAR\",\n",
    "                    \"China, Macao SAR\",\n",
    "                    \"China, Taiwan Province of China\",\n",
    "                    \"Dem. People's Republic of Korea\",\n",
    "                    \"Japan\",\n",
    "                    \"Mongolia\",\n",
    "                    \"Republic of Korea\",\n",
    "                    \"Afghanistan\",\n",
    "                    \"Bangladesh\",\n",
    "                    \"Bhutan\",\n",
    "                    \"India\",\n",
    "                    \"Iran (Islamic Republic of)\",\n",
    "                    \"Maldives\",\n",
    "                    \"Nepal\",\n",
    "                    \"Pakistan\",\n",
    "                    \"Sri Lanka\",\n",
    "                    \"Brunei Darussalam\",\n",
    "                    \"Cambodia\",\n",
    "                    \"Indonesia\",\n",
    "                    \"Lao People's Democratic Republic\",\n",
    "                    \"Malaysia\",\n",
    "                    \"Myanmar\",\n",
    "                    \"Philippines\",\n",
    "                    \"Singapore\",\n",
    "                    \"Thailand\",\n",
    "                    \"Timor-Leste\",\n",
    "                    \"Viet Nam\",\n",
    "                    \"Armenia\",\n",
    "                    \"Azerbaijan\",\n",
    "                    \"Bahrain\",\n",
    "                    \"Cyprus\",\n",
    "                    \"Georgia\",\n",
    "                    \"Iraq\",\n",
    "                    \"Israel\",\n",
    "                    \"Jordan\",\n",
    "                    \"Kuwait\",\n",
    "                    \"Lebanon\",\n",
    "                    \"Oman\",\n",
    "                    \"Qatar\",\n",
    "                    \"Saudi Arabia\",\n",
    "                    \"State of Palestine\",\n",
    "                    \"Syrian Arab Republic\",\n",
    "                    \"Türkiye\",\n",
    "                    \"United Arab Emirates\",\n",
    "                    \"Yemen\",\n",
    "                    \"Belarus\",\n",
    "                    \"Bulgaria\",\n",
    "                    \"Czechia\",\n",
    "                    \"Hungary\",\n",
    "                    \"Poland\",\n",
    "                    \"Republic of Moldova\",\n",
    "                    \"Romania\",\n",
    "                    \"Russian Federation\",\n",
    "                    \"Slovakia\",\n",
    "                    \"Ukraine\",\n",
    "                    \"Denmark\",\n",
    "                    \"Estonia\",\n",
    "                    \"Faroe Islands\",\n",
    "                    \"Finland\",\n",
    "                    \"Guernsey\",\n",
    "                    \"Iceland\",\n",
    "                    \"Ireland\",\n",
    "                    \"Isle of Man\",\n",
    "                    \"Jersey\",\n",
    "                    \"Latvia\",\n",
    "                    \"Lithuania\",\n",
    "                    \"Norway\",\n",
    "                    \"Sweden\",\n",
    "                    \"United Kingdom\",\n",
    "                    \"Albania\",\n",
    "                    \"Andorra\",\n",
    "                    \"Bosnia and Herzegovina\",\n",
    "                    \"Croatia\",\n",
    "                    \"Gibraltar\",\n",
    "                    \"Greece\",\n",
    "                    \"Holy See\",\n",
    "                    \"Italy\",\n",
    "                    \"Kosovo (under UNSC res. 1244)\",\n",
    "                    \"Malta\",\n",
    "                    \"Montenegro\",\n",
    "                    \"North Macedonia\",\n",
    "                    \"Portugal\",\n",
    "                    \"San Marino\",\n",
    "                    \"Serbia\",\n",
    "                    \"Slovenia\",\n",
    "                    \"Spain\",\n",
    "                    \"Austria\",\n",
    "                    \"Belgium\",\n",
    "                    \"France\",\n",
    "                    \"Germany\",\n",
    "                    \"Liechtenstein\",\n",
    "                    \"Luxembourg\",\n",
    "                    \"Monaco\",\n",
    "                    \"Netherlands\",\n",
    "                    \"Switzerland\",\n",
    "                    \"Anguilla\",\n",
    "                    \"Antigua and Barbuda\",\n",
    "                    \"Aruba\",\n",
    "                    \"Bahamas\",\n",
    "                    \"Barbados\",\n",
    "                    \"Bonaire, Sint Eustatius and Saba\",\n",
    "                    \"British Virgin Islands\",\n",
    "                    \"Cayman Islands\",\n",
    "                    \"Cuba\",\n",
    "                    \"Curaçao\",\n",
    "                    \"Dominica\",\n",
    "                    \"Dominican Republic\",\n",
    "                    \"Grenada\",\n",
    "                    \"Guadeloupe\",\n",
    "                    \"Haiti\",\n",
    "                    \"Jamaica\",\n",
    "                    \"Martinique\",\n",
    "                    \"Montserrat\",\n",
    "                    \"Puerto Rico\",\n",
    "                    \"Saint Barthélemy\",\n",
    "                    \"Saint Kitts and Nevis\",\n",
    "                    \"Saint Lucia\",\n",
    "                    \"Saint Martin (French part)\",\n",
    "                    \"Saint Vincent and the Grenadines\",\n",
    "                    \"Sint Maarten (Dutch part)\",\n",
    "                    \"Trinidad and Tobago\",\n",
    "                    \"Turks and Caicos Islands\",\n",
    "                    \"United States Virgin Islands\",\n",
    "                    \"Belize\",\n",
    "                    \"Costa Rica\",\n",
    "                    \"El Salvador\",\n",
    "                    \"Guatemala\",\n",
    "                    \"Honduras\",\n",
    "                    \"Mexico\",\n",
    "                    \"Nicaragua\",\n",
    "                    \"Panama\",\n",
    "                    \"Argentina\",\n",
    "                    \"Bolivia (Plurinational State of)\",\n",
    "                    \"Brazil\",\n",
    "                    \"Chile\",\n",
    "                    \"Colombia\",\n",
    "                    \"Ecuador\",\n",
    "                    \"Falkland Islands (Malvinas)\",\n",
    "                    \"French Guiana\",\n",
    "                    \"Guyana\",\n",
    "                    \"Paraguay\",\n",
    "                    \"Peru\",\n",
    "                    \"Suriname\",\n",
    "                    \"Uruguay\",\n",
    "                    \"Venezuela (Bolivarian Republic of)\",\n",
    "                    \"Bermuda\",\n",
    "                    \"Canada\",\n",
    "                    \"Greenland\",\n",
    "                    \"Saint Pierre and Miquelon\",\n",
    "                    \"United States of America\",\n",
    "                    \"Australia\",\n",
    "                    \"New Zealand\",\n",
    "                    \"Fiji\",\n",
    "                    \"New Caledonia\",\n",
    "                    \"Papua New Guinea\",\n",
    "                    \"Solomon Islands\",\n",
    "                    \"Vanuatu\",\n",
    "                    \"Guam\",\n",
    "                    \"Kiribati\",\n",
    "                    \"Marshall Islands\",\n",
    "                    \"Micronesia (Fed. States of)\",\n",
    "                    \"Nauru\",\n",
    "                    \"Northern Mariana Islands\",\n",
    "                    \"Palau\",\n",
    "                    \"American Samoa\",\n",
    "                    \"Cook Islands\",\n",
    "                    \"French Polynesia\",\n",
    "                    \"Niue\",\n",
    "                    \"Samoa\",\n",
    "                    \"Tokelau\",\n",
    "                    \"Tonga\",\n",
    "                    \"Tuvalu\",\n",
    "                    \"Wallis and Futuna Islands\",\n",
    "                    ]\n",
    "\n",
    "\n",
    "    checked = [name for name in list(codes.values()) if name not in UNreg]\n",
    "\n",
    "    if len(checked) > 0:\n",
    "        print (checked)\n",
    "        print (\"These countries are not mapped \", checked)\n",
    "\n",
    "\n",
    "    scenarios = {\"SSP3_v9_130115\"\t:\t\"SSP3\"\t,\n",
    "                \"SSP1_v9_130219\"\t:\t\"SSP1\"\t,\n",
    "                \"SSP2_v9_130219\"\t:\t\"SSP2\"\t,\n",
    "                \"SSP3_v9_130219\"\t:\t\"SSP3\"\t,\n",
    "                \"SSP4_v9_130219\"\t:\t\"SSP4\"\t,\n",
    "                \"SSP5_v9_130219\"\t:\t\"SSP5\"\t,\n",
    "                \"SSP1_v9_130325\"\t:\t\"SSP1\"\t,\n",
    "                \"SSP2_v9_130325\"\t:\t\"SSP2\"\t,\n",
    "                \"SSP3_v9_130325\"\t:\t\"SSP3\"\t,\n",
    "                \"SSP4_v9_130325\"\t:\t\"SSP4\"\t,\n",
    "                \"SSP5_v9_130325\"\t:\t\"SSP5\"\t,\n",
    "                \"SSP1_v9_130424\"\t:\t\"SSP1\"\t,\n",
    "                \"SSP2_v9_130424\"\t:\t\"SSP2\"\t,\n",
    "                \"SSP3_v9_130424\"\t:\t\"SSP3\"\t,\n",
    "                \"SSP4_v9_130424\"\t:\t\"SSP4\"\t,\n",
    "                \"SSP5_v9_130424\"\t:\t\"SSP5\"\t,\n",
    "                \"SSP5_v9_130115\"\t:\t\"SSP5\"\t,\n",
    "                \"SSP1_v9_130115\"\t:\t\"SSP1\"\t,\n",
    "                \"SSP2_v9_130115\"\t:\t\"SSP2\"\t,\n",
    "                \"SSP4_v9_130115\"\t:\t\"SSP4\"\t,\n",
    "                \"SSP4d_v9_130115\"\t:\t\"SSP4\"\t}\n",
    "\n",
    "    SSPpop = SSP.loc[SSP.VARIABLE == \"Population\"]\n",
    "    SSPgdp  = SSP.loc[SSP.VARIABLE == \"GDP|PPP\"]\n",
    "\n",
    "    SSPgroups = pd.DataFrame(SSPpop.groupby([\"MODEL\",\"SCENARIO\"]).sum(numeric_only=True).reset_index())\n",
    "    SSPgdpgroups  = pd.DataFrame(SSPgdp.groupby([\"MODEL\",\"SCENARIO\"]).sum(numeric_only=True).reset_index())\n",
    "    remove = \"PIK GDP-32\"\n",
    "    SSPgroups = pd.DataFrame(SSPgroups.loc[SSPgroups.MODEL != remove])\n",
    "    SSPgdpgroups = pd.DataFrame(SSPgdpgroups.loc[SSPgdpgroups.MODEL != remove])\n",
    "    for o in scenarios.keys():\n",
    "        SSPgroups[\"SCENARIO\"]  = SSPgroups[\"SCENARIO\"].replace(o, scenarios[o])\n",
    "        SSPgdpgroups[\"SCENARIO\"]  = SSPgdpgroups[\"SCENARIO\"].replace(o, scenarios[o])\n",
    "    SSPgroups[\"IPCCRegion\"] = \"WORLD\"\n",
    "    SSPgdpgroups[\"IPCCRegion\"] = \"WORLD\"\n",
    "    SSPgroups = SSPgroups.rename(columns={\"MODEL\": \"Model\", \"SCENARIO\": \"Scenario\"})\n",
    "    return (SSPpop, SSPgdp, SSPgroups, SSPgdpgroups, codes, scenarios)\n",
    "\n",
    "def counts (data: pd.DataFrame, region: str):\n",
    "\n",
    "    \"\"\"Returns number of population scenarios for each data input \n",
    "    and selected region in 2050 and 2100\n",
    "    data: IPCC database with each year in a single column\n",
    "    region: selected region\n",
    "\n",
    "    Returns: dataout: array with shape (2,1) with number of scenarios in\n",
    "    year 2050 and in year 2100\"\"\"\n",
    "\n",
    "    datavar = data.loc[((data.Variable == \"Population\") & (data.Region == region))]\n",
    "    datavar1 = datavar[\"2050\"][datavar[\"2050\"] > 0]\n",
    "    vals2050 = len(datavar1)\n",
    "    datavar2 = datavar[\"2100\"][datavar[\"2100\"] > 0]\n",
    "    vals2100 = len(datavar2)\n",
    "    name = list(set(data.Report))[0]\n",
    "    dataout = pd.DataFrame(np.array([vals2050, vals2100]).reshape(2,1), \n",
    "            columns=[\"Scenario Number\"], index=[name, name])\n",
    "    dataout[\"Region\"]=region\n",
    "    dataout[\"Year\"]=np.array([2050, 2100]).reshape(2,1)\n",
    "    return dataout\n",
    "\n",
    "def data_error(truedata: pd.DataFrame, \n",
    "               indata: pd.DataFrame, \n",
    "               year: float, \n",
    "               threshold: float,\n",
    "               region: str, \n",
    "               dataset: str,\n",
    "               scaler: int,\n",
    "               untype: str):\n",
    "\n",
    "    \"\"\"Gives error estimates on a regional scale assuming UN mean as true data\"\"\"\n",
    "    \"\"\"Estimate percent error dividing RMSE by true value\"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    truedata: UN population data with yearly data by columns\n",
    "    indata: AR population\n",
    "    year: selected year to estimate error\n",
    "    threshold: tuple with quantiles for keeping interquartile range\n",
    "    region: selected region\n",
    "    dataset: name of dataset\n",
    "    scaler: scaler used for data\n",
    "    untype: type of data (UN or SSP)\n",
    "    \n",
    "    Returns: mean absolute error, mean square error, \n",
    "    ratioed root mean squared error\"\"\"\n",
    "\n",
    "    y_pred = pd.DataFrame(indata.loc[indata.Year==year])[\"Population\"].values\n",
    "\n",
    "    if untype == \"UN\":\n",
    "        y_new = transformUN(truedata, region, False)\n",
    "    else:\n",
    "        y_new = transformSSP(truedata, region, [year], \"Population\")\n",
    "        y_new = y_new.rename(columns={y_new.columns[0]: \"Year\"})\n",
    "        y_new[\"Year\"] = year\n",
    "\n",
    "    y_new = pd.DataFrame(y_new.loc[(y_new.Year == year)])[\"Population\"].values\n",
    "\n",
    "    # transforms UN data into millions (when equal 1) or billions (when equal 1000)\n",
    "    y_new *= 1/scaler\n",
    "\n",
    "    # calculate error on the basis of UN mean and length of AR data\n",
    "    n = len(y_pred)\n",
    "\n",
    "    y_true = np.repeat(np.mean(y_new), n)\n",
    "\n",
    "    MAE = np.sum(np.abs(y_true - y_pred))/(n-1)\n",
    "    MSE = np.sum(np.power((y_true - y_pred),2))/(n-1)\n",
    "    RMSE = 100 * np.power(MSE, 0.5)/np.mean(y_true)\n",
    "\n",
    "    return MAE, MSE, RMSE\n",
    "\n",
    "def trimsample (sample: np.array,\n",
    "                low: float,\n",
    "                up: float):\n",
    "    \"\"\"Trims samples of data based on a defined percentage cut\n",
    "    Values below or higher the relative cut are filtered out\n",
    "    sample: array of data to trim, representing values for a year\n",
    "    low: relative percentage for cutting lower bounds\n",
    "    up: relative percentage for cutting higher bounds\n",
    "\n",
    "    Returns: array without trimmed values\"\"\"\n",
    "\n",
    "    # mask the values\n",
    "    trimmed = trim(sample, limits=(low, up), inclusive=(True, True), relative=True)\n",
    "    # masked values substituted with zeros\n",
    "    trimmed = trimmed.filled(0.0)\n",
    "    # filter out zeros\n",
    "    trimmed = trimmed[np.where(trimmed >0.0)]\n",
    "    return trimmed\n",
    "\n",
    "def pre_test (sample_0: np.array,\n",
    "              sample_1: np.array,  \n",
    "              low_0: float, \n",
    "              up_0: float, \n",
    "              low_1: float, \n",
    "              up_1: float, \n",
    "              function: str,\n",
    "              qvalue=0):\n",
    "\n",
    "    \"\"\"Pre-process samples before applying a Welch t-test\n",
    "    and calculates the function on the data to be used in the test,\n",
    "    for example a certain percentile or the standard deviation\n",
    "\n",
    "    sample_0: first sample of population data at a certain year, 1D array\n",
    "    sample_1: second sample of population data at a certain year, 1D array\n",
    "    low_0: relative percentage for the lower-bound trimming on first sample\n",
    "    up_0: relative percentage for the upper-bound trimming on first sample\n",
    "    low_1: relative percentage for the lower-bound trimming on second sample\n",
    "    up_1: relative percentage for the upper-bound trimming on second sample\n",
    "    function: statistics to calculate on the sample, \n",
    "    accepted values are: mean (\"mean\"), standard deviation (\"std\"), and percentile (\"percentile\")\n",
    "    qvalue: value of the calculated quantile, default to 0\n",
    "\n",
    "    Returns: \n",
    "    v1: calculated statistics on sample 1\n",
    "    s1: variance of sample 1 \n",
    "    n1: number of elements in sample 1\n",
    "    v2: calculated statistics on sample 2\n",
    "    s2: variance of sample 2\n",
    "    n2: number of elements in sample 2\n",
    "    \"\"\"\n",
    "\n",
    "    sample1 = trimsample(sample_0, low_0, up_0)\n",
    "    sample2 = trimsample(sample_1, low_1, up_1)\n",
    "    sample1 = np.reshape(np.asarray(sample1),-1)\n",
    "    sample2 = np.reshape(np.asarray(sample2),-1)\n",
    "    if function==\"mean\":\n",
    "        v1 = np.mean(sample1)\n",
    "        v2 = np.mean(sample2)\n",
    "    if function==\"std\":\n",
    "        v1 = np.std(sample1)\n",
    "        v2 = np.std(sample2)\n",
    "    if function==\"percentile\":\n",
    "        v1 = np.quantile(sample1, qvalue)\n",
    "        v2 = np.quantile(sample2, qvalue)\n",
    "\n",
    "    s1 = np.var(sample1)\n",
    "    s2 = np.var(sample2)\n",
    "    n1 = sample1.shape[0]\n",
    "    n2 = sample2.shape[0]\n",
    "\n",
    "    return (v1, s1, n1, v2, s2, n2)\n",
    "\n",
    "def welch_test (v1: float,\n",
    "                s1: float, \n",
    "                n1: int, \n",
    "                v2: float, \n",
    "                s2: float, \n",
    "                n2: int, \n",
    "                alternative: str):\n",
    "\n",
    "    \"\"\"Applies Welch t-test for two samples or for a sample and value\n",
    "    v1: calculated statistics on sample 1\n",
    "    s1: variance of sample 1 \n",
    "    n1: number of elements in sample 1\n",
    "    v2: calculated statistics on sample 2 \n",
    "    s2: variance of sample 2\n",
    "    n2: number of elements in sample 2\n",
    "\n",
    "    Returns:\n",
    "    welch-test result, the degrees of freedom, and the p-value\n",
    "    \"\"\"\n",
    "    vn1 = v1 / n1\n",
    "    vn2 = v2 / n2\n",
    "    denom = np.sqrt(vn1 + vn2)\n",
    "\n",
    "    if n2 > 1:\n",
    "        df = np.power((vn1 + vn2),2) / (np.power((vn1),2) / (n1 -1) + np.power((vn2),2) / (n2-1))\n",
    "    else:\n",
    "        df = np.power((s1/n1 + s2/n2),2) / (np.power((s1/n1),2) / (n1 -1) )\n",
    "\n",
    "    d = v1-v2\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        welch_t = np.divide(d, denom)\n",
    "    if alternative == 'less':\n",
    "        pval = special.stdtr(df, welch_t)\n",
    "    elif alternative == 'greater':\n",
    "        pval = special.stdtr(df, -welch_t)\n",
    "    elif alternative == 'two-sided':\n",
    "        pval = special.stdtr(df, -np.abs(welch_t))*2\n",
    "\n",
    "    return (welch_t, df, pval)\n",
    "\n",
    "def databdiff(database1: pd.DataFrame,\n",
    "             name1: str, \n",
    "             database2: pd.DataFrame, \n",
    "             name2: str,\n",
    "             variable: str,\n",
    "             function: str,\n",
    "             qvalue: float,\n",
    "             alternative: str,\n",
    "             low_1: float,\n",
    "             up_1: float,\n",
    "             low_2: float,\n",
    "             up_2: float,\n",
    "             years: list):\n",
    "    \"\"\"Represents differences between database 1 and database 2\n",
    "    distributing them in percentage of decrease or increase\n",
    "    database1: data sample 1\n",
    "    name1: name of sample 1 database\n",
    "    database2: data sample 2\n",
    "    name2: name of sample 2 database\n",
    "    variable: variable to compare\n",
    "    function: statistics to calculate on the sample,\n",
    "    qvalue: value of the calculated quantile, default to 0\n",
    "    alternative: alternative hypothesis, accepted values are: less, greater, and two-sided\n",
    "    low_1: relative percentage for the lower-bound trimming on first sample\n",
    "    up_1: relative percentage for the upper-bound trimming on first sample\n",
    "    low_2: relative percentage for the lower-bound trimming on second sample\n",
    "    up_2:  relative percentage for the upper-bound trimming on second sample\n",
    "    years: list of years to compare\n",
    "    \"\"\"\n",
    "\n",
    "    out = pd.DataFrame()\n",
    "\n",
    "    for year in years:\n",
    "        sample1 = database1.loc[database1.Year==year][variable]\n",
    "        sample1 = pd.DataFrame(sample1)\n",
    "\n",
    "        sample2 = database2.loc[database2.Year==year][variable]\n",
    "        sample2 = pd.DataFrame(sample2)\n",
    "\n",
    "        if function==\"mean\":\n",
    "            mndict = {variable: np.mean(sample2, axis=0)}\n",
    "            mn2 = pd.DataFrame.from_dict(mndict, orient=\"columns\").reset_index(drop=True)\n",
    "\n",
    "        if function==\"std\":\n",
    "            mndict = {variable: np.std(sample2, axis=0).values[0]}\n",
    "            mn2 = pd.DataFrame.from_dict(mndict, orient='index', columns=[variable])\n",
    "\n",
    "        if function==\"percentile\":\n",
    "            mn2 = pd.DataFrame(data=np.array(sample2[variable].quantile(qvalue)).reshape(1,1), index=None, columns=[variable])\n",
    "\n",
    "        #vv = np.array([-0.99, -0.5, -0.10, -0.01, 0.01, 0.10, 0.50, 0.99])\n",
    "        vv = np.array([-0.1,-0.01, 0.01, 0.1])\n",
    "\n",
    "        # we compare the dataset deviations with \n",
    "        # a fraction of the calculated statistics\n",
    "        # to determine the bins\n",
    "\n",
    "        labels = list([ \"medium-to-high decrease\",  \n",
    "                        \"low decrease\", \"nearest\", \"low increase\",\n",
    "                        \"medium-to-high increase\"])\n",
    "        \n",
    "        bins = mn2.values * vv\n",
    "        bins = bins[0]\n",
    "        # differences (left) AR6 value vs the AR5/SR 1.5 (right)\n",
    "        # -0.5\t0.5\n",
    "        # -0.1\t0.9\n",
    "        # -0.01\t0.99\n",
    "        # 0.01\t1.01\n",
    "        # 0.1\t1.1\n",
    "        # 0.5\t1.5\n",
    "        # mar6 - mar5 <= -100 mar5\n",
    "        # mar6 <= -99 mar5 \n",
    "        # database2 multiplier: -0.1 + 1 = 0.9, -0.01 + 1 = 0.99,  + 0.01  + 1 = 1.10,  +0.1 + 1 = 1.1\n",
    "\n",
    "        v_bins = pd.IntervalIndex.from_tuples([ (100 * bins[0], bins[0]),\n",
    "                                                (bins[0], bins[1]),\n",
    "                                                (bins[1], bins[2]),\n",
    "                                                (bins[2], bins[3]),\n",
    "                                                (bins[3], 100 * bins[3])])\n",
    "\n",
    "        arr = sample1 - mn2.values\n",
    "\n",
    "\n",
    "\n",
    "        arr[\"value\"] = pd.cut(arr.values.flatten(), v_bins, right=True, \n",
    "                                labels=labels, retbins=False, \n",
    "                                precision=3, include_lowest=False, \n",
    "                                duplicates='raise', ordered=True)\n",
    "\n",
    "        grouped = arr.groupby(\"value\", observed=False).count()\n",
    "        \n",
    "        data = pd.DataFrame(grouped.transpose().values, columns=labels)\n",
    "\n",
    "        v1, s1, n1, v2, s2, n2 = pre_test(sample1, sample2, low_1, up_1, low_2, up_2, function, qvalue)\n",
    "        \n",
    "        welch_t, df, pval = welch_test(v1, s1, n1, v2, s2, n2, alternative)\n",
    "\n",
    "        p_value = np.round(pval,3)\n",
    "\n",
    "        data[\"p_value\"] = p_value\n",
    "\n",
    "        data[\"year\"] = year\n",
    "\n",
    "        out = pd.concat((out, data), axis=0)\n",
    "\n",
    "    return out\n",
    "\n",
    "def ext_census(years: list):\n",
    "    \"\"\"\n",
    "    Uploads external datasets for the analysis\n",
    "    US Census Data in billions of people\n",
    "\n",
    "    years: years to consider\n",
    "\n",
    "    Returns external datasets:\n",
    "    censusg: U.S. Census data for global population (billion)\n",
    "    \"\"\"\n",
    "    folder = r'data/other_pop_data'\n",
    "    filename1 = \"uscensus.xlsx\"\n",
    "\n",
    "    census = pd.read_excel(os.path.join(folder, filename1),\"uscensus\")\n",
    "    excluded = [\"Annual Growth Rate %\",\n",
    "            \"Area (sq km)\",\n",
    "            \"Density (per sq km)\",\n",
    "            \"Total Fertility Rate\",\n",
    "            \"Life Expectancy at Birth\",\n",
    "            \"Under-5 Mortality Rate\"]\n",
    "    included = [col for col in census.columns if col not in excluded]\n",
    "    census = census[included]\n",
    "    census = census.loc[census.Year.isin(years)]\n",
    "    censusg = census.groupby([\"Year\", \"IPCCRegion\"])[\"Population\"].sum().reset_index()\n",
    "    censusg[\"Model\"] = \"U.S. Census\"\n",
    "    censusg[\"Series\"] = \"U.S. Census\"\n",
    "    censusg[\"Scenario\"] = \"Median USC\"\n",
    "    #censusg[\"Type\"] = \"Determ\"\n",
    "\n",
    "    world = census.groupby([\"Year\"])[\"Population\"].sum().reset_index()\n",
    "    world[\"IPCCRegion\"] = \"WORLD\"\n",
    "    world[\"Model\"] = \"U.S. Census\"\n",
    "    world[\"Series\"] = \"U.S. Census\"\n",
    "    world[\"Scenario\"] = \"Median USC\"\n",
    "    #world[\"Type\"] = \"Determ\"\n",
    "    world = world[censusg.columns]\n",
    "    world[\"Population\"] = world[\"Population\"] / 1e9\n",
    "\n",
    "    return world\n",
    "\n",
    "def ext_wbank(updtcolumns: list, years: list):\n",
    "        \"\"\"Extracts World Bank data\n",
    "        \n",
    "        Inputs:\n",
    "        columns: list of columns to order dataframe\n",
    "        years: list of years to consider\n",
    "        \n",
    "        Returns:\n",
    "        wbankg: pd DataFrame with global population (billion)\n",
    "        \"\"\"\n",
    "        # upload world bank data\n",
    "\n",
    "        folder = r'data/other_pop_data'\n",
    "        filename2 = \"worldbank.xlsx\"\n",
    "        syears = [str(year) for year in years]\n",
    "        columns = [\"Country Name\", \"IPCCRegion\"]\n",
    "\n",
    "        newcolumns = columns + syears\n",
    "        syears = [[str(year) + \" \" + \"[YR\" + str(year) + \"]\"] for year in years]\n",
    "        syears = [item for sublist in syears for item in sublist]\n",
    "\n",
    "        columns = [\"Country Name\", \"IPCCRegion\"] + syears\n",
    "\n",
    "        dtypes = [str, str, float, float, float]\n",
    "        wbank = pd.read_excel(os.path.join(folder, filename2),\"worldbank\", \n",
    "        usecols=columns)\n",
    "        wbank = wbank.rename(columns=dict(zip(columns,newcolumns)))\n",
    "        wbank = pd.DataFrame(wbank[newcolumns])\n",
    "        IPCCRegions = [\"R5ASIA\", \"R5OECD90+EU\", \"R5OWO\", \"R5LAM\", \"R5MAF\", \"R5REF\"]\n",
    "        wbank = wbank[wbank.IPCCRegion.isin(IPCCRegions)]\n",
    "\n",
    "        wbankg = pd.DataFrame()\n",
    "        for y in newcolumns[2:]:\n",
    "            added = pd.DataFrame(wbank.groupby(\"IPCCRegion\")[y].sum().reset_index())\n",
    "            added = added.rename(columns={y:\"Population1\"})\n",
    "            added[\"Year\"] = int(y)\n",
    "            wbankg = pd.concat((wbankg,added), axis=0)\n",
    "        wbankg = wbankg.loc[wbankg.IPCCRegion != \"IPCCRegion\"]\n",
    "\n",
    "        world = wbankg.groupby([\"Year\"])[\"Population1\"].sum().reset_index()\n",
    " \n",
    "        world[\"IPCCRegion\"] = \"WORLD\"\n",
    "        world[\"Model\"] = \"World Bank\"\n",
    "        world[\"Type\"] = \"Determ\"\n",
    "        world[\"Series\"] = \"WorldBank\"\n",
    "        world[\"Scenario\"] = \"Median WB\"\n",
    "        world[\"Population\"] = world[\"Population1\"] * 1/1e9\n",
    "\n",
    "        return world[updtcolumns]\n",
    "\n",
    "def ext_ihme(updtcolumns: list, years: list):\n",
    "    \"\"\"Extracts IHME data\n",
    "    from: \n",
    "    https://ghdx.healthdata.org/record/ihme-data/global-population-forecasts-2017-2100\n",
    "\n",
    "    Inputs:\n",
    "    columns: list of columns to order dataframe\n",
    "    years: list of years to extract\n",
    "    \n",
    "    Returns:\n",
    "    ihmeframe: pd DataFrame with global population (billion)\n",
    "    \"\"\"\n",
    "    \n",
    "    # upload sdg data\n",
    "    folder = r'data/other_pop_data'\n",
    "    filename = \"IHME.csv\"\n",
    "\n",
    "    datar = pd.read_csv(os.path.join(folder, filename))\n",
    "\n",
    "    data = datar.groupby([\"location_name\",\"year_id\", \"scenario_name\"])[[\"val\", \"upper\", \"lower\"]].sum().reset_index()\n",
    "    data = data.rename(columns={\"location_name\": \"IPCCRegion\", \"year_id\": \"Year\", \"val\": \"Population\", \"scenario_name\": \"Series\"})\n",
    "    data[\"Model\"] = \"IHME\"\n",
    "    data[\"Type\"] = \"Determ\"\n",
    "    data[\"Scenario\"] = data[\"Series\"]\n",
    "\n",
    "    ihmeframe = data.loc[data.IPCCRegion == \"Global\"]\n",
    "    ihmeframe = ihmeframe.replace({\"Global\": \"WORLD\"})\n",
    "    ihmeframe[\"Population\"] *= 1/1e9\n",
    "    ihmeframe = ihmeframe.loc[ihmeframe[\"Year\"].isin(years)]\n",
    "    ihmeframe = ihmeframe[updtcolumns]\n",
    "    return (ihmeframe)\n",
    "\n",
    "def ext_RFF(years: list):\n",
    "    \"\"\"\"\n",
    "    Extract Resources For the Future (RFF) database\n",
    "    Receives:\n",
    "    years: list of years to extract\n",
    "    Returns:\n",
    "    pdRFF: pd.DataFrame with percentiles of RFF distribution (billion)\n",
    "    \"\"\"\n",
    "    ### Upload RFF data\n",
    "    folder=r'data'\n",
    "    all_data = pd.read_csv(os.path.join(folder,\"RFF.csv\"))\n",
    "\n",
    "    ### Comparison between probabilistic projections RFF with UN percentiles\n",
    "\n",
    "    # Here values are transformed in billions\n",
    "    all_data = all_data.rename(columns={\"Pop\": \"Population\"})\n",
    "\n",
    "    quantiles = [0.05, 0.2, 0.5, 0.8, 0.95]\n",
    "    # selected data \n",
    "    sdata = pd.concat([pd.DataFrame(all_data[[\"Population\",\"year\"]].loc[all_data.year==y]) for y in years])\n",
    "\n",
    "    # estimate quantiles per year and collect everything in undata\n",
    "    pdRFF = pd.DataFrame()\n",
    "    for y in years:\n",
    "        pdquant = pd.concat([pd.DataFrame.quantile(pd.DataFrame(sdata[[\"Population\"]].loc[all_data.year==y])/1e6, quantile) for quantile in quantiles] )\n",
    "        pdquant=pd.concat((pdquant.reset_index(),pd.Series(np.array(quantiles)).reset_index()),axis=1)\n",
    "        pdquant[\"Year\"] = y\n",
    "        pdRFF = pd.concat((pdRFF,pdquant), axis=0)\n",
    "    pdRFF = pdRFF.drop(\"index\", axis=1)\n",
    "    pdRFF.columns = [\"Population\", \"Series\", \"Year\"]\n",
    "    pdRFF[\"Scenario\"] = pdRFF[\"Series\"]\n",
    "    change = {0.05: \"5 pRFF\", 0.2: \"20 pRFF\", 0.5: \"50 pRFF\", 0.8: \"80 pRFF\", 0.95: \"95 pRFF\"}\n",
    "    for ch in change.keys():\n",
    "        pdRFF[\"Scenario\"] = pdRFF[\"Scenario\"].replace(ch, change[ch])\n",
    "    pdRFF[\"Model\"] = \"RFF\"\n",
    "    return pdRFF\n",
    "    \n",
    "def ext_undata(wprob, wdet, columns):\n",
    "    \"\"\"Extracts the undata probabilistic\n",
    "    and the extreme deterministic scenarios\n",
    "    in vertical format\n",
    "\n",
    "    Inputs:\n",
    "    wprob: dataframe with probabilistic projections from UN\n",
    "    wdet: dataframe with deterministic scenarios from UN\n",
    "    columns: same ase pdRFF columns\n",
    "\n",
    "    Returns:\n",
    "    undata: dataframe with probabilistic projections from UN with year by row\n",
    "    undata_ext:  dataframe with extreme deterministic projections\n",
    "    Values are in billion and reported with each year row by row\n",
    "    \"\"\"\n",
    "    # UN data of a certain revision\n",
    "    undata = transformUN(wprob, \"WORLD\", True)\n",
    "    undata[\"Model\"] = \"UN\"\n",
    "    # change = {\"5\": \"5 pUN\", \"20\": \"20 pUN\", \"50\": \"50 pUN\", \"80\": \"80 pUN\", \"95\": \"95 pUN\"}\n",
    "    # undata = undata[columns]\n",
    "    # for ch in change.keys():\n",
    "    #     undata[\"Scenario\"] = undata[\"Scenario\"].replace(ch, change[ch])\n",
    "    undata[\"Series\"] = pd.Series(undata[\"Scenario\"])\n",
    "    # change = {\"5\": 0.05, \"20\": 0.2, \"50\": 0.5, \"80\": 0.8, \"95\": 0.95}\n",
    "    # for ch in change.keys():\n",
    "    #     undata[\"Series\"] = undata[\"Series\"].replace(ch, change[ch])\n",
    "    # undata=undata.sort_values(by=\"Series\")\n",
    "    # undata = undata.drop(\"Series\",axis=1)\n",
    "    undata[\"Population\"] *= 1/1e3\n",
    "    undata[\"Model\"]=\"pUN\"\n",
    "\n",
    "    ### Max/min deterministic scenarios from UN, year\n",
    "    region=\"WORLD\"\n",
    "    undata_ext, sel_labels = transformUNminmax(wdet, region, False)\n",
    "    undata_ext[\"Population\"] *=1/1000\n",
    "    undata_ext[\"Model\"]=\"dUN\"\n",
    "    return undata, undata_ext\n",
    "\n",
    "def maxestimate(SSPUN, undata2022, censusg, wbankg, ihmeframe):\n",
    "    \"\"\"Estimates maximum of selected timeseries\n",
    "    International Database generated by U.S. Census (IDB), WOrld Bank, \n",
    "    SSP1, SSP2, SSP3, SSP4, SSP5\n",
    "    UN percentiles of distribution (95, 50, 25, 5)\n",
    "    Institution of Health and Metrics Evaluation (IHME) scenarios,\n",
    "    of faster and  lower growwth of educational atttanment anf health quality\n",
    "    Receives:\n",
    "    SSPUN: DataFrame with SSP and extreme deterministic UN\n",
    "    undata2022: DataFrame with probabilistic UN data\n",
    "    censusg: DataFrame with IDB data\n",
    "    wbankg: DataFrame with World Bank data\n",
    "    ihmeframe: DataFrame with IHME\n",
    "    \"\"\"\n",
    "    datassp1 = pd.DataFrame(SSPUN.loc[ ((SSPUN.Model==\"SSP\") & (SSPUN.Scenario==\"SSP1\")) ])\n",
    "    datassp2 = pd.DataFrame(SSPUN.loc[ ((SSPUN.Model==\"SSP\") & (SSPUN.Scenario==\"SSP2\")) ])\n",
    "    datassp3 = pd.DataFrame(SSPUN.loc[ ((SSPUN.Model==\"SSP\") & (SSPUN.Scenario==\"SSP3\")) ])\n",
    "    datassp4 = pd.DataFrame(SSPUN.loc[ ((SSPUN.Model==\"SSP\") & (SSPUN.Scenario==\"SSP4\")) ])\n",
    "    datassp5 = pd.DataFrame(SSPUN.loc[ ((SSPUN.Model==\"SSP\") & (SSPUN.Scenario==\"SSP5\")) ])\n",
    "\n",
    "    undata95 = pd.DataFrame(undata2022.loc[ (undata2022.Series==\"95\") ])\n",
    "    undata50 = pd.DataFrame(undata2022.loc[ (undata2022.Series==\"50\") ])\n",
    "    undata25 = pd.DataFrame(undata2022.loc[ (undata2022.Series==\"20\") ])\n",
    "    undata5 = pd.DataFrame(undata2022.loc[ (undata2022.Series==\"5\") ])\n",
    "\n",
    "    dataun = SSPUN.loc[SSPUN.Model==\"dUN\"]\n",
    "    datalo = dataun.loc[dataun.Scenario ==\"Low variant\"]\n",
    "    datact = dataun.loc[dataun.Scenario == \"Constant-fertility\"]\n",
    "\n",
    "    ihme = ihmeframe.copy(deep=True)\n",
    "    ihme = ihme.replace({\"Faster Met Need and Education\": \"Faster\", \n",
    "                        \"Fastest Met Need and Education\": \"Fastest\",\n",
    "                        \"Reference\": \"Reference\",\n",
    "                        \"SDG Met Need and Education\": \"SDG\",\n",
    "                        \"Slower Met Need and Education\": \"Slower\"})\n",
    "\n",
    "    datafaster = pd.DataFrame(ihme.loc[ (ihme.Scenario==\"Faster\") ])\n",
    "    datafastest = pd.DataFrame(ihme.loc[ (ihme.Scenario==\"Fastest\") ])\n",
    "    dataref = pd.DataFrame(ihme.loc[ (ihme.Scenario==\"Reference\") ])\n",
    "    datasdg = pd.DataFrame(ihme.loc[ (ihme.Scenario==\"SDG\") ])\n",
    "    dataslower = pd.DataFrame(ihme.loc[ (ihme.Scenario==\"Slower\") ])\n",
    "\n",
    "    data = [censusg, wbankg, datassp1, datassp2, datassp3, datassp4, datassp5, undata95, undata50, undata25, undata5,\n",
    "    ]\n",
    "\n",
    "    names = [\"IDB\", \"World Bank\", \"SSP1\", \"SSP2\", \"SSP3\", \"SSP4\", \"SSP5\", \"pUN 95 perc\", \"pUN 50 perc\", \"pUN 25 perc\", \"pUN 5 perc\"]\n",
    "    for d,dd in enumerate(data):\n",
    "        m = max(dd[\"Population\"])\n",
    "        arg = pd.DataFrame(dd.loc[dd.Population == m][\"Year\"]).values[0]\n",
    "        m = round(m, 3)\n",
    "        print  (names[d], \" has maximum of \", m, \" billion inhabitants in year \", arg[0])\n",
    "\n",
    "\n",
    "    data = [datalo, datact]\n",
    "    names =[\"UN low variant\", \"UN Costant fertility\"]\n",
    "\n",
    "    for d,dd in enumerate(data):\n",
    "        m = max(dd[\"Population\"])\n",
    "        arg = pd.DataFrame(dd.loc[dd.Population == m][\"Year\"]).values[0]\n",
    "        m = round(m, 3)\n",
    "        print  (names[d], \" has maximum of \", m, \" billion inhabitants in year \", arg[0])\n",
    "\n",
    "    data = [ datafaster, datafastest, dataref, datasdg, dataslower]\n",
    "    names = [\"Faster\", \"Fastest\", \"Reference\", \"SDG\", \"Lower\"]\n",
    "\n",
    "    for d,dd in enumerate(data):\n",
    "        m = max(dd[\"Population\"])\n",
    "        arg = pd.DataFrame(dd.loc[dd.Population == m][\"Year\"]).values[0]\n",
    "        m = round(m, 3)\n",
    "        print  (\"Scenario \", names[d], \" has maximum of \", m, \" billion inhabitants in year \", arg[0])\n",
    "      \n",
    "def plot_ars(ars: pd.DataFrame, \n",
    "            undata1: pd.DataFrame,\n",
    "            undata2: pd.DataFrame,\n",
    "            region: str,\n",
    "            scenarios1: list,\n",
    "            label_scenarios1: list,\n",
    "            scenarios2: list,\n",
    "            label_scenarios2: list,\n",
    "            limits: tuple,\n",
    "            ylabel: str,\n",
    "            years: list):\n",
    "    \"\"\"Plots 1 dataframes in two boxplots (AR)\n",
    "    and 2 dataframes as overlaid curves\n",
    "    on each subplot (ie. UN databases and SSPs)\n",
    "    alongisde subplot with marginals for 2100\n",
    "    ars: dataframe with AR data\n",
    "    undata1: dataframe with data for first subplot (UN)\n",
    "    undata2: dataframe with data for second subplot (SSPs)\n",
    "    region: region to plot\n",
    "    scenarios1: list of scenarios to plot in first subplot\n",
    "    label_scenarios1: list of labels for scenarios in first subplot\n",
    "    scenarios2: list of scenarios to plot in second subplot\n",
    "    label_scenarios2: list of labels for scenarios in second subplot\n",
    "    limits: tuple with limits for y axis\n",
    "    ylabel: label for y axis\"\"\"\n",
    "\n",
    "    cmap = cm.get_cmap('tab20')\n",
    "    bar_colors_neg1 = cmap(np.linspace(0, 1, int(np.ceil(len(scenarios1))+1)))\n",
    "    bar_colors1 = bar_colors_neg1\n",
    "\n",
    "\n",
    "    cmap = cm.get_cmap('tab20c')\n",
    "    bar_colors_neg2 = cmap(np.linspace(0, 1, int(np.ceil(len(scenarios2))+1)))\n",
    "    bar_colors2 = bar_colors_neg2\n",
    "\n",
    "    ar2100 = pd.DataFrame(ars.loc[ars.Year==2100])\n",
    "\n",
    "    fig = plt.figure()\n",
    "\n",
    "    gs = fig.add_gridspec(1, 2)\n",
    "    fig.set_tight_layout(True)\n",
    "    a1 = fig.add_subplot(gs[0])\n",
    "\n",
    "    flierprops = dict(marker='o', markerfacecolor=\"y\", markersize=3,\n",
    "                    linestyle='none',markeredgecolor='teal')\n",
    "\n",
    "    ystring = [str(y) for y in years]\n",
    "    \n",
    "    for sc, scenario in enumerate(scenarios1):   \n",
    "        w = pd.DataFrame(undata1.loc[undata1.Scenario==scenario]) \n",
    "        w[\"Year\"] = np.linspace(1, 9, 9)\n",
    "        a1.x = np.linspace(1, 9, 9)\n",
    "        a1.y = w.Population\n",
    "        a1.plot(a1.x, a1.y, color=bar_colors1[sc], label= sc)\n",
    "\n",
    "\n",
    "    ars.boxplot(column=\"Population\", ax= a1,  by=\"Year\", rot=90, \n",
    "        grid=False,  flierprops=flierprops)\n",
    "    xlabels = [\"\", \"2030\", \"\", \"2050\", \"\", \"2070\", \"\", \"2090\", \" \"]\n",
    "    a1.set_xticklabels(xlabels)\n",
    "    a1.set_title(\"UN percentiles\")\n",
    "\n",
    "    plt.ylabel(ylabel)\n",
    "    a1.set_ylim(limits)\n",
    "    plt.legend(labels=label_scenarios1)\n",
    "\n",
    "    a2 = fig.add_subplot(gs[1])\n",
    "\n",
    "\n",
    "    for sc, scenario in enumerate(scenarios2):\n",
    "        w = undata2.loc[undata2.Scenario==scenario].groupby([\"Year\", \"Scenario\"])[\"Population\"].mean().reset_index()\n",
    "        w[\"Model\"] = \"SSP\"\n",
    "        w[\"Year\"]  = np.linspace(1, 9, 9)\n",
    "        a2.x = np.linspace(1, 9, 9)\n",
    "        a2.y = w.Population\n",
    "        a2.plot(a2.x, a2.y, color=bar_colors2[sc], label= sc)\n",
    "\n",
    "\n",
    "    ars.boxplot(column=\"Population\", ax= a2,  by=\"Year\", rot=90, \n",
    "        grid=False, flierprops=flierprops)\n",
    "    a2.set_title(\"SSPs\")\n",
    "    plt.legend(labels=label_scenarios2)\n",
    "    a2.set_ylim(limits)\n",
    "\n",
    "    a2.set_xticklabels(xlabels)\n",
    "    plt.ylabel(ylabel)\n",
    "    fig.get_figure().suptitle(region, fontsize=16)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_2ars(ars: pd.DataFrame, \n",
    "            undata1: pd.DataFrame,\n",
    "            undata2: pd.DataFrame,\n",
    "            region: str,\n",
    "            scenarios1: list,\n",
    "            label_scenarios1: list,\n",
    "            scenarios2: list,\n",
    "            label_scenarios2: list,\n",
    "            limits: tuple,\n",
    "            ylabel: str,\n",
    "            years: list,):\n",
    "    \"\"\"Plots 2 dataframes in two boxplots (AR)\n",
    "    and another dataframe as overlaid curves\n",
    "    on each subplot (ie. UN databases and SSPs)\n",
    "    alongisde subplot with marginals for 2100\n",
    "    ars: dataframe with AR data\n",
    "    undata1: dataframe with data for first subplot (UN)\n",
    "    undata2: dataframe with data for second subplot (SSPs)\n",
    "    region: region to plot\n",
    "    scenarios1: list of scenarios to plot in first subplot\n",
    "    label_scenarios1: list of labels for scenarios in first subplot\n",
    "    scenarios2: list of scenarios to plot in second subplot\n",
    "    label_scenarios2: list of labels for scenarios in second subplot\n",
    "    limits: tuple with limits for y axis\n",
    "    ylabel: label for y axis\n",
    "    \"\"\"\n",
    "\n",
    "    from matplotlib.transforms import Affine2D\n",
    "    import mpl_toolkits.axisartist.floating_axes as floating_axes\n",
    "    cmap= cm.get_cmap(\"tab20b\")\n",
    "    bar_colors_neg1 = cmap(np.linspace(0, 1, int(np.ceil(len(scenarios1))) ))\n",
    "    bar_colors1 = bar_colors_neg1\n",
    "\n",
    "\n",
    "    cmap = cm.get_cmap(\"nipy_spectral\")\n",
    "    bar_colors_neg2 = cmap(np.linspace(0, 1, int(np.ceil(len(scenarios2))) ))\n",
    "    bar_colors2=bar_colors_neg2 \n",
    "    # np.vstack((bar_colors_neg2[2:], bar_colors_neg1[3:]))\n",
    "    \n",
    "\n",
    "    ar2100 = pd.DataFrame(ars.loc[ars.Year==2100])\n",
    "\n",
    "    fig = plt.figure()\n",
    "\n",
    "    gs = fig.add_gridspec(1, 3,)\n",
    "   \n",
    "    a1 = fig.add_subplot(gs[0])\n",
    "\n",
    "    flierprops = dict(marker='o', markerfacecolor=\"y\", markersize=1.5,\n",
    "                    linestyle='none',markeredgecolor='teal')\n",
    "\n",
    "    ystring = [str(y) for y in years]\n",
    "    \n",
    "    for sc, scenario in enumerate(scenarios1):   \n",
    "        w = pd.DataFrame(undata1.loc[undata1.Scenario==scenario])  \n",
    "        w[\"Year\"] = ystring \n",
    "        a1.x = np.linspace(1, 9, 9)\n",
    "        a1.y = w.Population\n",
    "        a1.plot(a1.x, a1.y, color=bar_colors1[sc], label= sc)\n",
    "\n",
    "\n",
    "    ars.boxplot(column=\"Population\", ax= a1,  by=\"Year\", rot=90, \n",
    "        grid=False,  flierprops=flierprops)\n",
    "    xlabels = [\"\", \"2030\", \"\", \"2050\", \"\", \"2070\", \"\", \"2090\", \" \"]\n",
    "    a1.set_xticklabels(xlabels)\n",
    "    a1.set_title(\"UN percentiles\")\n",
    "    a1.tick_params(axis=\"x\",labelrotation=45, pad=0.2)\n",
    "    \n",
    "    plt.ylabel(ylabel)\n",
    "    a1.set_ylim(limits)\n",
    "    plt.legend(labels=label_scenarios1)\n",
    "    a2 = fig.add_subplot(gs[1])\n",
    "\n",
    "\n",
    "    for sc, scenario in enumerate(scenarios2):\n",
    "        w = undata2.loc[undata2.Scenario==scenario].groupby([\"Year\", \"Scenario\"])[\"Population\"].mean().reset_index()\n",
    "        w[\"Model\"] = \"SSP\"\n",
    "        w[\"Year\"] = ystring\n",
    "        a2.x = np.linspace(1, 9, 9)\n",
    "        a2.y = w.Population\n",
    "        a2.plot(a2.x, a2.y, color=bar_colors2[sc], label= sc)\n",
    "\n",
    "\n",
    "    ars.boxplot(column=\"Population\", ax= a2, by=\"Year\", rot=90, \n",
    "        grid=False, flierprops=flierprops)\n",
    "    a2.set_title(\"SSPs\")\n",
    "    plt.legend(labels=label_scenarios2)\n",
    "    a2.set_ylim(limits)\n",
    "\n",
    "    a2.set_xticklabels(xlabels)\n",
    "    a2.set_yticklabels([])\n",
    "    a2.tick_params(axis=\"x\",labelrotation=45, pad=0.2)\n",
    "\n",
    "    a3 = fig.add_subplot(gs[2])\n",
    "\n",
    "    sns.kdeplot(\n",
    "        data=pd.DataFrame(ar2100), y=\"Population\", fill=True, ax=a3,\n",
    "\n",
    "    )\n",
    "\n",
    "    a3.set_ylabel(\"\")   \n",
    "    a3.set_yticklabels([])\n",
    "    a3.set_xticks([0, 0.5, 1.0, 1.5], labels=[\" \", \"0.5\", \"1.0\", \"1.5\"])\n",
    "    a3.tick_params(axis=\"x\",labelrotation=45, pad=7)\n",
    "    fig.get_figure().suptitle(\" \")\n",
    "    \n",
    "    plt.title(\"Year=2100\")\n",
    "    plt.ylim(limits)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(hspace=.0)\n",
    "    plt.show()\n",
    "    return(fig)\n",
    "\n",
    "def plot_mars(ars: pd.DataFrame, \n",
    "            undata1: pd.DataFrame,\n",
    "            region: str,\n",
    "            scenarios1: list,\n",
    "            label_scenarios1: list,\n",
    "            limits: tuple,\n",
    "            ylabel: str,\n",
    "            years: list):\n",
    "    \"\"\"Plots 2 dataframes in two boxplots (AR)\n",
    "    and another dataframe as overlaid curves\n",
    "    on each subplot (ie. UN databases and SSPs)\n",
    "    alongisde subplot with marginals for 2100\n",
    "    ars: dataframe with AR data\n",
    "    undata1: dataframe with data for first subplot (UN)\n",
    "    region: region to plot\n",
    "    scenarios1: list of scenarios to plot in first subplot\n",
    "    label_scenarios1: list of labels for scenarios in first subplot\n",
    "    limits: tuple with limits for y axis\n",
    "    ylabel: label for y axis\"\"\"\n",
    "\n",
    "    cmap = cm.get_cmap('tab20')\n",
    "    bar_colors_neg1 = cmap(np.linspace(0, 1, int(np.ceil(len(scenarios1))) + 2 ))\n",
    "    bar_colors1 = bar_colors_neg1\n",
    "\n",
    "    ar2100 = pd.DataFrame(ars.loc[ars.Year==2100])\n",
    "\n",
    "    fig = plt.figure()\n",
    "\n",
    "    gs = fig.add_gridspec(1, 2)\n",
    "    fig.set_tight_layout(True)\n",
    "    a1 = fig.add_subplot(gs[0])\n",
    "\n",
    "    flierprops = dict(marker='o', markerfacecolor=\"y\", markersize=1.5,\n",
    "                    linestyle='none',markeredgecolor='teal')\n",
    "\n",
    "    ystring = [str(y) for y in years]\n",
    "    \n",
    "    for sc, scenario in enumerate(scenarios1):\n",
    "        w = undata1.loc[undata1.Scenario==scenario].groupby([\"Year\", \"Scenario\"])[\"Population\"].mean().reset_index()\n",
    "        w[\"Model\"] = \"SSP\"\n",
    "        w[\"Year\"] = ystring\n",
    "        a1.x = np.linspace(1, 9, 9)\n",
    "        a1.y = w.Population\n",
    "        a1.plot(a1.x, a1.y, color=bar_colors1[sc], label= sc)\n",
    "\n",
    "\n",
    "    ars.boxplot(column=\"Population\", ax= a1,  by=\"Year\", rot=90, \n",
    "        grid=False,  flierprops=flierprops)\n",
    "    xlabels = [\"\", \"2030\", \"\", \"2050\", \"\", \"2070\", \"\", \"2090\", \" \"]\n",
    "    a1.set_xticklabels(xlabels)\n",
    "    a1.set_title(\"SSPS\")\n",
    "\n",
    "    plt.ylabel(ylabel)\n",
    "    a1.set_ylim(limits)\n",
    "    plt.legend(labels=label_scenarios1)\n",
    "    \n",
    "    a2 = fig.add_subplot(gs[1])\n",
    "    sns.kdeplot(\n",
    "        data=ar2100, y=\"Population\",  fill=True,\n",
    "    )\n",
    "    a2.set_xlabel(ylabel)\n",
    "\n",
    "    fig.get_figure().suptitle(region, fontsize=16)\n",
    "    \n",
    "    plt.title(\"Year=2100\")\n",
    "    plt.ylim(limits)\n",
    "\n",
    "    plt.show()\n",
    "    return(fig)\n",
    "\n",
    "path_for_figure = os.path.join(os.getcwd(),\"figures_population_paper\")\n",
    "osExists = os.path.exists(path_for_figure)\n",
    "\n",
    "if not osExists:\n",
    "    os.makedirs(path_for_figure)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('manet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "72d48519e37cc5ab56142f71b0b165c1e93899565b658f2927699b7e50952d16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
